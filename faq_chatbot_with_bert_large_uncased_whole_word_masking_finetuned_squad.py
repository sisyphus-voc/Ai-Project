# -*- coding: utf-8 -*-
"""FAQ chatbot with BERT-large-uncased-whole-word-masking-finetuned-squad.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_x13t8JLuZSXmHs8ZYkPRBM4PNPDyR61

# Required Libraries
We need this libraries to work with the dataset
"""

!pip install transformers pandas scikit-learn torch

"""# Loading the Dataset
We have carefully curated (feature extraction) the FAQs dataset of 1000+ records which we will enhance further
"""

import pandas as pd

# Loading the FAQ dataset from CSV
faq_df = pd.read_csv('faq_dataset.csv')  # Replace 'faq_data.csv' with your file path

# Converting the dataset to a list of dictionaries
faq_dataset = faq_df.to_dict(orient="records")

# Display the first few records to ensure proper loading
print(faq_df.head())

"""# BERT Model and Tokenizer Set Up
We are load the pre-trained BERT model bert-large-uncased-whole-word-masking-finetuned-squadand its tokenizer. This model is fine-tuned for question-answering tasks. This is really large and efficient model with over 340M parameters.
"""

from transformers import BertTokenizer, BertForQuestionAnswering

# Loading the pre-trained BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')
model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

# Confirming that the model and tokenizer are loaded successfully
print("BERT Model and Tokenizer Loaded.")

"""# Implementing Question Answering Pipeline
We're using the BERT model to answer questions based on the context (FAQ entries). The context will be dynamically generated for each question-answer pair in the dataset.
"""

from transformers import pipeline

# Initializing the Question Answering pipeline
qa_pipeline = pipeline("question-answering", model=model, tokenizer=tokenizer)

# Example question and context from the dataset
test_context = faq_dataset[0]["Answer"]
test_question = "What services do you offer?"
answer = qa_pipeline(question=test_question, context=test_context)

# Display the answer
print(f"Answer: {answer['answer']}")

"""# Function to Retrieve Answers
Function get_answer processes the user query and finds the most relevant answer from the FAQ dataset.
"""

# Function to get the most relevant answer from the FAQ dataset
def get_answer(user_query):
    # List to store possible answers with their confidence scores
    answers = []

    # Loop through each FAQ entry in the dataset
    for faq in faq_dataset:
        context = f"Category: {faq['Category']} Question: {faq['Question']} Answer: {faq['Answer']}"

        # Use BERT to get the answer to the user's query based on the context
        result = qa_pipeline(question=user_query, context=context)
        answers.append((faq['Question'], result['answer'], result['score']))  # Store question, answer, and score

    # Sort answers by the confidence score (highest first)
    answers = sorted(answers, key=lambda x: x[2], reverse=True)

    # If the highest score is below a certain threshold, return a generic response
    if answers[0][2] < 0.5:  # You can adjust this threshold based on testing
        return "I'm sorry, I couldn't find an answer to your question."

    # Return the most relevant answer
    return f"Question: {answers[0][0]}\nAnswer: {answers[0][1]}"

"""# Interactive Chat Loop
Chatbot loop where the user can type queries, and the chatbot will respond with the most relevant answer.
"""

# Interactive loop for user to query
print("Welcome to the FAQ Chatbot! Type your question or 'exit' to quit.")
while True:
    # Take user input
    user_query = input("You: ")

    # Exit condition
    if user_query.lower() == "exit":
        print("Goodbye!")
        break

    # Get the most relevant answer from the FAQ dataset
    answer = get_answer(user_query)
    print(f"Chatbot: {answer}")